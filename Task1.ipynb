# ---------------------------------------------------
# Codetech Internship: Machine Learning - Task 1
# Decision Tree Classifier with Visualization
# Dataset: Iris
# ---------------------------------------------------

# ğŸ“Œ Step 1: Import Necessary Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# ğŸ“Œ Step 2: Load the Dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name="target")

# ğŸ“Œ Step 3: Display Dataset Information
print("Dataset Shape:", X.shape)
print("Feature Names:", X.columns.tolist())
print("Target Classes:", iris.target_names)
print("\nSample Data:\n", X.head())

# ğŸ“Œ Step 4: Exploratory Data Analysis (EDA)
plt.figure(figsize=(12, 8))
sns.pairplot(pd.concat([X, y], axis=1), hue="target", palette="Set1")
plt.suptitle("Feature Distribution by Class", y=1.02)
plt.show()

# Heatmap of Correlation
plt.figure(figsize=(8,6))
sns.heatmap(X.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# ğŸ“Œ Step 5: Data Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
print("Training size:", X_train.shape)
print("Test size:", X_test.shape)

# ğŸ“Œ Step 6: Baseline Decision Tree Model
baseline_model = DecisionTreeClassifier(random_state=42)
baseline_model.fit(X_train, y_train)

# ğŸ“Œ Step 7: Model Evaluation
y_pred = baseline_model.predict(X_test)

print("\nğŸ” Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nğŸ“Š Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# ğŸ“Œ Step 8: Visualize the Decision Tree
plt.figure(figsize=(16, 10))
plot_tree(baseline_model,
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True,
          fontsize=12)
plt.title("Baseline Decision Tree")
plt.show()

# ğŸ“Œ Step 9: Tree as Text
tree_rules = export_text(baseline_model, feature_names=iris.feature_names)
print("\nğŸ“ Decision Tree Rules:\n", tree_rules)

# ğŸ“Œ Step 10: Cross-Validation
scores = cross_val_score(baseline_model, X, y, cv=5)
print("\nğŸ“ˆ Cross-Validation Accuracy (5-fold):")
print("Scores:", scores)
print("Mean Accuracy:", scores.mean())

# ğŸ“Œ Step 11: Hyperparameter Tuning with GridSearchCV
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [2, 3, 4, 5, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_model = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, n_jobs=-1, verbose=1)
grid_model.fit(X_train, y_train)

print("\nâœ… Best Parameters Found:", grid_model.best_params_)

# ğŸ“Œ Step 12: Evaluate Best Model
best_model = grid_model.best_estimator_
y_pred_best = best_model.predict(X_test)

print("\nğŸ§  Best Model Accuracy:", accuracy_score(y_test, y_pred_best))
print("\nğŸ§¾ Classification Report:\n", classification_report(y_test, y_pred_best))

# ğŸ“Œ Step 13: Visualize Best Decision Tree
plt.figure(figsize=(16, 10))
plot_tree(best_model,
          feature_names=iris.feature_names,
          class_names=iris.target_names,
          filled=True,
          rounded=True,
          fontsize=12)
plt.title("Optimized Decision Tree")
plt.show()

# ğŸ“Œ Step 14: Export Model as .dot or .pkl (optional)
import joblib
joblib.dump(best_model, "decision_tree_iris_model.pkl")
print("âœ… Model saved as decision_tree_iris_model.pkl")

# ğŸ“Œ Step 15: Decision Boundary Plot (for 2 features only)
from matplotlib.colors import ListedColormap

def plot_decision_boundary(X, y, model, title="Decision Boundary"):
    X = X.values[:, :2]  # use first 2 features for visualization
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, cmap=cmap_light)
    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette=cmap_bold, edgecolor='k')
    plt.xlabel("Sepal length (cm)")
    plt.ylabel("Sepal width (cm)")
    plt.title(title)
    plt.show()

plot_decision_boundary(X.iloc[:, :2], y, DecisionTreeClassifier(max_depth=3).fit(X.iloc[:, :2], y),
                       "Decision Tree Decision Boundary (Sepal Features)")
